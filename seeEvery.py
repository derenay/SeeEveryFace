import gradio as gr
from ultralytics import YOLO
from deepface import DeepFace
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import os
import json
import cv2
import time 
import shutil 
import pandas as pd 
import tempfile

# --- Ayarlar ---
K_SHOTS_ENROLLMENT_MANUAL = 3
MAX_IMAGES_PER_PERSON_FS = 150
EMBEDDING_MODEL_NAME = 'Facenet512' # Facenet512
YOLO_FACE_DETECTION_MODEL = 'project-02/yolov11m-face.pt' 
DEEPFACE_DETECTOR_BACKEND_FOR_CROPS = 'skip'
DEFAULT_DISTANCE_THRESHOLD = 0.50 
PROTOTYPES_FILE = 'face_prototypes_database_v4_facenet512.json' 
DEFAULT_DATASET_BASE_PATH = "project-02/gelen_fotograflar/Human" 

try:
    FONT_PATH = "arial.ttf" if os.name == 'nt' else "DejaVuSans.ttf"
    ImageFont.truetype(FONT_PATH, 15)
except IOError:
    FONT_PATH = None
    print(f"UyarÄ±: Belirtilen font ('{FONT_PATH if FONT_PATH else 'arial.ttf/DejaVuSans.ttf'}') bulunamadÄ±. VarsayÄ±lan font kullanÄ±lacak.")


enrolled_prototypes = {} # {'isim': {'prototype_vector': np.array, 'num_samples': int, 'source': 'manual'/'filesystem'}}
yolo_model = None
deepface_ready = False


def initialize_models():
    global yolo_model, deepface_ready
    if yolo_model is None:
        try:
            yolo_model = YOLO(YOLO_FACE_DETECTION_MODEL)
            print(f"YOLO modeli '{YOLO_FACE_DETECTION_MODEL}' baÅŸarÄ±yla yÃ¼klendi.")
        except Exception as e:
            print(f"HATA: YOLO modeli yÃ¼klenemedi: {e}")
    if not deepface_ready:
        try:
            dummy_img_for_warmup = np.zeros((64, 64, 3), dtype=np.uint8)
            DeepFace.represent(dummy_img_for_warmup, model_name=EMBEDDING_MODEL_NAME, enforce_detection=False, detector_backend=DEEPFACE_DETECTOR_BACKEND_FOR_CROPS)
            deepface_ready = True
            print(f"DeepFace embedding modeli '{EMBEDDING_MODEL_NAME}' kullanÄ±ma hazÄ±r.")
        except Exception as e:
            print(f"HATA: DeepFace modeli '{EMBEDDING_MODEL_NAME}' yÃ¼klenemedi/kullanÄ±lamadÄ±: {e}")

def load_font(size=15):
    if FONT_PATH:
        try:
            return ImageFont.truetype(FONT_PATH, size)
        except IOError: pass
    try: return ImageFont.load_default(size=size)
    except TypeError: return ImageFont.load_default()

def save_prototypes_to_json():
    global enrolled_prototypes
    try:
        with open(PROTOTYPES_FILE, 'w', encoding='utf-8') as f:
            json_friendly_prototypes = {
                name: {
                    'prototype_vector': data['prototype_vector'].tolist(),
                    'num_samples': data['num_samples'],
                    'source': data.get('source', 'unknown')
                } for name, data in enrolled_prototypes.items()
            }
            json.dump(json_friendly_prototypes, f, indent=4, ensure_ascii=False)
        print(f"Prototip veritabanÄ± '{PROTOTYPES_FILE}' dosyasÄ±na kaydedildi.")
        return f"{len(enrolled_prototypes)} kiÅŸi kayÄ±tlÄ±. VeritabanÄ± gÃ¼ncellendi."
    except Exception as e:
        print(f"HATA: Prototip veritabanÄ± kaydedilirken: {e}")
        return f"HATA: VeritabanÄ± kaydedilemedi: {e}"

def load_prototypes_from_json():
    global enrolled_prototypes
    if os.path.exists(PROTOTYPES_FILE):
        try:
            with open(PROTOTYPES_FILE, 'r', encoding='utf-8') as f:
                json_friendly_prototypes = json.load(f)
                temp_prototypes = {}
                for name, data in json_friendly_prototypes.items():
                    temp_prototypes[name] = {
                        'prototype_vector': np.array(data['prototype_vector']),
                        'num_samples': data['num_samples'],
                        'source': data.get('source', 'manual')
                    }
                enrolled_prototypes = temp_prototypes 
            print(f"Prototip veritabanÄ± '{PROTOTYPES_FILE}' dosyasÄ±ndan yÃ¼klendi ({len(enrolled_prototypes)} kiÅŸi).")
        except Exception as e:
            enrolled_prototypes = {} 
            print(f"HATA: Prototip veritabanÄ± yÃ¼klenirken: {e}")
    else:
        enrolled_prototypes = {}
        print(f"'{PROTOTYPES_FILE}' bulunamadÄ±. KayÄ±tlÄ± kiÅŸi yok (JSON).")

def get_face_crop_from_image_pil(image_pil, yolo_detector_instance):
    if yolo_detector_instance is None: return None
    try:
        results = yolo_detector_instance(image_pil, verbose=False, conf=0.3)
    except Exception as e:
        print(f"YOLO ile yÃ¼z tespiti sÄ±rasÄ±nda hata: {e}")
        return None
        
    best_box = None; max_area = 0
    for result in results:
        for box_data in result.boxes:
            if box_data.conf[0] > 0.3:
                x1, y1, x2, y2 = map(int, box_data.xyxy[0].tolist())
                area = (x2 - x1) * (y2 - y1)
                if area > max_area: max_area = area; best_box = (x1, y1, x2, y2)
    if best_box:
        img_width, img_height = image_pil.size
        x1_c,y1_c,x2_c,y2_c = max(0,best_box[0]),max(0,best_box[1]),min(img_width,best_box[2]),min(img_height,best_box[3])
        if x1_c < x2_c and y1_c < y2_c: return image_pil.crop((x1_c, y1_c, x2_c, y2_c))
    return None

def get_embedding_from_face_pil(face_crop_pil):
    global deepface_ready
    if not deepface_ready or face_crop_pil is None or face_crop_pil.width < 20 or face_crop_pil.height < 20: return None
    try:
        face_crop_numpy_rgb = np.array(face_crop_pil.convert('RGB'))
        face_crop_numpy_bgr = cv2.cvtColor(face_crop_numpy_rgb, cv2.COLOR_RGB2BGR)
        embedding_objs = DeepFace.represent(
            img_path=face_crop_numpy_bgr, model_name=EMBEDDING_MODEL_NAME,
            enforce_detection=False, detector_backend=DEEPFACE_DETECTOR_BACKEND_FOR_CROPS
        )
        if embedding_objs and len(embedding_objs) > 0: return np.array(embedding_objs[0]["embedding"])
        return None
    except Exception as e:
        if "Face could not be detected" not in str(e) and "Singleton array array" not in str(e) and "expected BGR image" not in str(e).lower():
             print(f"Embedding Ã§Ä±karÄ±lÄ±rken hata: {e} (YÃ¼z boyutu: {face_crop_pil.size})")
        return None

def calculate_cosine_distance_numpy(vec1, vec2):
    if vec1 is None or vec2 is None: return float('inf')
    norm_vec1 = np.linalg.norm(vec1); norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0: return float('inf')
    return 1 - (np.dot(vec1, vec2) / (norm_vec1 * norm_vec2))

def scan_filesystem_and_enroll_interface(dataset_base_path_str, progress=gr.Progress(track_tqdm=True)):
    global enrolled_prototypes, yolo_model
    if not dataset_base_path_str or not os.path.isdir(dataset_base_path_str):
        return "Hata: GeÃ§ersiz klasÃ¶r yolu.", pd.DataFrame()
    if yolo_model is None or not deepface_ready:
        return "Hata: Ana modeller (YOLO/DeepFace) yÃ¼klenemedi.", pd.DataFrame()

    progress(0, desc="KlasÃ¶rler taranÄ±yor...")
    person_folders = [d for d in os.listdir(dataset_base_path_str) if os.path.isdir(os.path.join(dataset_base_path_str, d))]
    total_folders = len(person_folders)
    fs_prototypes_found = {}
    processed_person_count = 0

    for i, person_name in enumerate(person_folders):
        progress((i + 1) / total_folders, desc=f"'{person_name}' iÅŸleniyor...")
        person_dir = os.path.join(dataset_base_path_str, person_name)
        embeddings_list = []
        images_processed_count = 0
        image_files_in_folder = [f for f in os.listdir(person_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        for image_file in sorted(image_files_in_folder):
            if images_processed_count >= MAX_IMAGES_PER_PERSON_FS: break
            image_path = os.path.join(person_dir, image_file)
            try:
                img_pil = Image.open(image_path).convert("RGB")
                face_crop_pil = get_face_crop_from_image_pil(img_pil, yolo_model)
                if face_crop_pil:
                    embedding = get_embedding_from_face_pil(face_crop_pil)
                    if embedding is not None:
                        embeddings_list.append(embedding)
                        images_processed_count += 1
            except Exception as e:
                print(f"    Hata: '{image_file}' iÅŸlenirken ({person_name}): {e}")
        
        if embeddings_list:
            prototype = np.mean(embeddings_list, axis=0)
            fs_prototypes_found[person_name] = {
                'prototype_vector': prototype,
                'num_samples': len(embeddings_list),
                'source': 'filesystem'
            }
            processed_person_count += 1
    
    for name, data in fs_prototypes_found.items():
        enrolled_prototypes[name] = data # Eski FS kayÄ±tlarÄ±nÄ± veya manuel olanlarÄ± gÃ¼nceller/ekler
    
    save_prototypes_to_json()
    status_message = f"{processed_person_count} kiÅŸi dosya sisteminden iÅŸlendi/gÃ¼ncellendi. Toplam {len(enrolled_prototypes)} kayÄ±tlÄ± kiÅŸi."
    print(status_message)
    return status_message, get_enrolled_people_dataframe() # GÃ¼ncellenmiÅŸ DataFrame'i dÃ¶ndÃ¼r

# --- Manuel KayÄ±t ---
def enroll_person_manual_interface(person_name_manual, img1_pil, img2_pil, img3_pil):
    global enrolled_prototypes
    if not deepface_ready: return "Hata: DeepFace modeli hazÄ±r deÄŸil.", get_enrolled_people_dataframe()
    if not person_name_manual.strip(): return "Hata: KiÅŸi adÄ± boÅŸ olamaz.", get_enrolled_people_dataframe()

    images_pil = [img for img in [img1_pil, img2_pil, img3_pil] if img is not None]
    if len(images_pil) != K_SHOTS_ENROLLMENT_MANUAL:
        return f"Hata: LÃ¼tfen tam olarak {K_SHOTS_ENROLLMENT_MANUAL} adet yÃ¼z fotoÄŸrafÄ± yÃ¼kleyin.", get_enrolled_people_dataframe()

    embeddings_list = []
    for i, img_pil in enumerate(images_pil):
        embedding = get_embedding_from_face_pil(img_pil)
        if embedding is not None: embeddings_list.append(embedding)
        else: return f"Hata: {i+1}. fotoÄŸraftan yÃ¼z Ã¶zelliÄŸi Ã§Ä±karÄ±lamadÄ±.", get_enrolled_people_dataframe()

    if len(embeddings_list) == K_SHOTS_ENROLLMENT_MANUAL:
        prototype = np.mean(embeddings_list, axis=0)
        enrolled_prototypes[person_name_manual] = {
            'prototype_vector': prototype,
            'num_samples': len(embeddings_list),
            'source': 'manual'
        }
        save_prototypes_to_json()
        return f"'{person_name_manual}' manuel olarak kaydedildi/gÃ¼ncellendi.", get_enrolled_people_dataframe()
    else: return "Hata: Yeterli geÃ§erli yÃ¼z Ã¶zelliÄŸi Ã§Ä±karÄ±lamadÄ±.", get_enrolled_people_dataframe()

# --- KayÄ±tlÄ± KiÅŸi YÃ¶netimi ---
def get_enrolled_people_dataframe():
    global enrolled_prototypes
    if not enrolled_prototypes:
        return pd.DataFrame(columns=["Ä°sim", "Ã–rnek SayÄ±sÄ±", "Kaynak"])
    data = []
    for name, info in enrolled_prototypes.items():
        data.append([name, info['num_samples'], info.get('source', 'Bilinmiyor')])
    return pd.DataFrame(data, columns=["Ä°sim", "Ã–rnek SayÄ±sÄ±", "Kaynak"])

def get_enrolled_people_names():
    return list(enrolled_prototypes.keys())

def delete_person_interface(person_name_to_delete):
    global enrolled_prototypes
    if not person_name_to_delete:
        return "Silinecek kiÅŸi seÃ§ilmedi.", get_enrolled_people_dataframe(), gr.Dropdown(choices=get_enrolled_people_names())
    if person_name_to_delete in enrolled_prototypes:
        del enrolled_prototypes[person_name_to_delete]
        save_prototypes_to_json()
        msg = f"'{person_name_to_delete}' adlÄ± kiÅŸi silindi."
        print(msg)
        return msg, get_enrolled_people_dataframe(), gr.Dropdown(choices=get_enrolled_people_names(), label="Silinecek KiÅŸiyi SeÃ§in")
    else:
        return f"Hata: '{person_name_to_delete}' adlÄ± kiÅŸi bulunamadÄ±.", get_enrolled_people_dataframe(), gr.Dropdown(choices=get_enrolled_people_names())


def process_frame_and_get_annotations(frame_pil, current_distance_threshold):
    global enrolled_prototypes, yolo_model
    if yolo_model is None or not deepface_ready: return frame_pil, "Hata: Modeller yÃ¼klenemedi.", []
    if not enrolled_prototypes: return frame_pil, "KayÄ±tlÄ± kiÅŸi yok.", []

    try:
        results = yolo_model.predict(frame_pil, verbose=False, conf=0.4)
    except Exception as e:
        print(f"YOLO ile yÃ¼z tespiti sÄ±rasÄ±nda hata (process_frame): {e}")
        return frame_pil, f"YOLO hatasÄ±: {e}", []

    annotated_frame_pil = frame_pil.copy()
    draw = ImageDraw.Draw(annotated_frame_pil)
    font_size = max(12, int(frame_pil.height / 45))
    font = load_font(size=font_size)
    raw_annotations = []; recognition_info_list = []

    for result_set in results:
        for box_data in result_set.boxes:
            x1,y1,x2,y2 = map(int, box_data.xyxy[0].tolist())
            face_crop_pil = frame_pil.crop((x1,y1,x2,y2))
            if face_crop_pil.width<20 or face_crop_pil.height<20: continue
            query_embedding = get_embedding_from_face_pil(face_crop_pil)
            identity="Bilinmeyen"; min_distance=float('inf'); color="red"; best_match_name_for_unknown="Yok"

            if query_embedding is not None:
                for name, data in enrolled_prototypes.items():
                    distance = calculate_cosine_distance_numpy(query_embedding, data['prototype_vector'])
                    if distance < min_distance: min_distance=distance; best_match_name_for_unknown=name
                if min_distance < current_distance_threshold:
                    identity=best_match_name_for_unknown; color="lime"
                    recognition_info_list.append(f"TanÄ±nan: {identity} (Mesafe: {min_distance:.3f})")
                else:
                    recognition_info_list.append(f"Bilinmeyen (En yakÄ±n: {best_match_name_for_unknown}, Mesafe: {min_distance:.3f})")
            else:
                recognition_info_list.append(f"YÃ¼zden Ã¶zellik Ã§Ä±karÄ±lamadÄ± ({x1},{y1}).")
                min_distance=float('inf')
            
            label_text = f"{identity} ({min_distance:.2f})" if query_embedding is not None else "Ã–zellik Yok"
            raw_annotations.append(((x1,y1,x2,y2), label_text, color))
            draw.rectangle([(x1,y1),(x2,y2)], outline=color, width=max(1,int(frame_pil.height/250)))
            try: text_bbox=draw.textbbox((x1,y1-font_size-2),label_text,font=font); text_w,text_h=text_bbox[2]-text_bbox[0],text_bbox[3]-text_bbox[1]
            except AttributeError: text_w,text_h=draw.textlength(label_text,font=font) if font else (70,10), font_size if font else 10
            bg_y1 = y1-text_h-4 if (y1-text_h-4)>0 else y1+2
            draw.rectangle([(x1,bg_y1),(x1+text_w+4,bg_y1+text_h+2)], fill=color)
            draw.text((x1+2,bg_y1),label_text, fill="black", font=font)
            
    summary_text = "\n".join(recognition_info_list) if recognition_info_list else "AnlamlÄ± yÃ¼z bulunamadÄ±/iÅŸlenemedi."
    return annotated_frame_pil, summary_text, raw_annotations

# --- Resimden TanÄ±ma ---
def recognize_faces_image_interface(input_image_pil, current_distance_threshold):
    if input_image_pil is None: return None, "Hata: GiriÅŸ resmi alÄ±namadÄ±."
    annotated_pil_frame, summary_text, _ = process_frame_and_get_annotations(input_image_pil, float(current_distance_threshold))
    return annotated_pil_frame, summary_text

def recognize_faces_video_interface(video_path_input, current_distance_threshold, progress=gr.Progress(track_tqdm=True)):
    if video_path_input is None: return None, "Hata: Video dosyasÄ± yÃ¼klenmedi."
    video_path = video_path_input # Gradio geÃ§ici dosya yolu verir

    if not os.path.exists(video_path):
        return None, f"Hata: Video dosyasÄ± bulunamadÄ±: {video_path}"

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None, f"Hata: Video dosyasÄ± aÃ§Ä±lamadÄ±: {video_path}"

    fps = cap.get(cv2.CAP_PROP_FPS)
    # FPS 0 ise veya Ã§ok yÃ¼ksekse makul bir deÄŸere ayarla (Ã¶rn: 30)
    if fps == 0 or fps > 120:  # BazÄ± hatalÄ± videolarda fps 0 veya Ã§ok yÃ¼ksek olabilir
        print(f"UyarÄ±: Videonun FPS deÄŸeri ({fps}) geÃ§ersiz gÃ¶rÃ¼nÃ¼yor. VarsayÄ±lan olarak 30 FPS kullanÄ±lacak.")
        fps = 30.0

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # EÄŸer width veya height 0 ise, ilk kareden okumayÄ± dene
    if width == 0 or height == 0:
        ret_tmp, frame_tmp = cap.read()
        if ret_tmp:
            height, width, _ = frame_tmp.shape
            print(f"UyarÄ±: Video baÅŸlÄ±k bilgisi eksik, ilk kareden boyutlar alÄ±ndÄ±: {width}x{height}")
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0) # Video okuyucuyu baÅŸa sar
        else:
            cap.release()
            return None, "Hata: Video boyutlarÄ± okunamadÄ±."


    # Python'un tempfile modÃ¼lÃ¼ ile gÃ¼venli bir geÃ§ici dosya oluÅŸtur
    # delete=False Ã¶nemlidir, bÃ¶ylece Gradio dosyayÄ± okuyana kadar silinmez.
    # Gradio dosyayÄ± kendi Ã¶nbelleÄŸine aldÄ±ktan sonra bu geÃ§ici dosya Ã¶nemsizleÅŸir.
    # Gradio genellikle kendi geÃ§ici dosyalarÄ±nÄ± kendi yÃ¶netir.
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmpfile:
        output_video_temp_path = tmpfile.name
    
    print(f"GeÃ§ici Ã§Ä±ktÄ± video dosyasÄ±: {output_video_temp_path}")

    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # veya 'avc1' H.264 iÃ§in
    out_writer = cv2.VideoWriter(output_video_temp_path, fourcc, fps, (width,height))
    
    if not out_writer.isOpened():
        cap.release()
        print(f"HATA: VideoWriter oluÅŸturulamadÄ±: {output_video_temp_path}")
        # EÄŸer dosya sisteminde yazma izni yoksa veya codec desteklenmiyorsa bu hata olabilir.
        # FarklÄ± bir fourcc deneyebilirsiniz: fourcc = cv2.VideoWriter_fourcc(*'XVID') (daha yaygÄ±n)
        # Veya fourcc = 0 (sistem varsayÄ±lanÄ±nÄ± kullanÄ±r, platforma baÄŸlÄ±)
        # fourcc = cv2.VideoWriter_fourcc('M','J','P','G') # MJPEG, daha bÃ¼yÃ¼k dosyalar ama daha uyumlu
        return None, f"Hata: Ã‡Ä±ktÄ± videosu ({output_video_temp_path}) iÃ§in VideoWriter oluÅŸturulamadÄ±. Codec veya yazma izni sorunu olabilir."


    frame_counter = 0
    last_known_raw_annotations = [] 
    all_summaries = []

    # total_frames 0 veya hatalÄ±ysa, progress.tqdm dÃ¼zgÃ¼n Ã§alÄ±ÅŸmayabilir.
    # Bu durumda sadece desc gÃ¶sterir.
    iterable_frames = range(total_frames) if total_frames > 0 else iter(int, 1) # Sonsuz dÃ¶ngÃ¼ deÄŸil, cap.read() ile kontrol

    for i in progress.tqdm(iterable_frames, desc="Video iÅŸleniyor...", total=total_frames if total_frames > 0 else None):
        ret, frame_cv2_bgr = cap.read()
        if not ret: 
            if total_frames == 0 : break # total_frames bilinmiyorsa ve okuma bittiyse Ã§Ä±k
            # EÄŸer total_frames biliniyorsa ve daha erken bittiyse, tqdm'in sonuna kadar gitmesine gerek yok
            print(f"UyarÄ±: Video beklenenden erken bitti. Okunan kare: {frame_counter}, beklenen: {total_frames}")
            break

        frame_counter += 1
        current_frame_pil_rgb = Image.fromarray(cv2.cvtColor(frame_cv2_bgr, cv2.COLOR_BGR2RGB))
        output_pil_frame_to_write = current_frame_pil_rgb.copy()

        if frame_counter % 4 == 0:
            annotated_pil_frame, summary, raw_annotations = process_frame_and_get_annotations(current_frame_pil_rgb, float(current_distance_threshold))
            if raw_annotations: last_known_raw_annotations = raw_annotations
            if summary and "Hata:" not in summary and "bulunmamaktadÄ±r" not in summary and "tespit edilemedi" not in summary:
                 all_summaries.append(f"Kare {frame_counter}: {summary}")
            output_pil_frame_to_write = annotated_pil_frame 
        else:
            if last_known_raw_annotations:
                draw = ImageDraw.Draw(output_pil_frame_to_write)
                font_size = max(12,int(height/45)); font = load_font(size=font_size)
                for (x1_ann,y1_ann,x2_ann,y2_ann),label_text,color in last_known_raw_annotations: # DeÄŸiÅŸken adlarÄ± Ã§akÄ±ÅŸmasÄ±n
                    draw.rectangle([(x1_ann,y1_ann),(x2_ann,y2_ann)], outline=color, width=max(1,int(height/250)))
                    try: text_bbox=draw.textbbox((x1_ann,y1_ann-font_size-2),label_text,font=font); text_w,text_h=text_bbox[2]-text_bbox[0],text_bbox[3]-text_bbox[1]
                    except AttributeError: text_w,text_h=draw.textlength(label_text,font=font) if font else (70,10), font_size if font else 10
                    bg_y1_ann = y1_ann-text_h-4 if (y1_ann-text_h-4)>0 else y1_ann+2 # DeÄŸiÅŸken adlarÄ±
                    draw.rectangle([(x1_ann,bg_y1_ann),(x1_ann+text_w+4,bg_y1_ann+text_h+2)],fill=color)
                    draw.text((x1_ann+2,bg_y1_ann),label_text,fill="black",font=font)
        
        out_writer.write(cv2.cvtColor(np.array(output_pil_frame_to_write), cv2.COLOR_RGB2BGR))
    
    cap.release()
    out_writer.release()
    
    final_summary_text = "\n---\n".join(all_summaries) if all_summaries else "Videoda Ã¶nemli bir tanÄ±ma yapÄ±lmadÄ± veya tÃ¼m kareler atlandÄ±/iÅŸlenemedi."
    if frame_counter == 0 and total_frames > 0 : # HiÃ§ kare okunamadÄ±ysa
        print(f"HATA: Videodan hiÃ§ kare okunamadÄ±: {video_path}")
        return None, "Hata: Video dosyasÄ± okunamadÄ± veya boÅŸ."

    print(f"Video iÅŸleme tamamlandÄ±. Ã‡Ä±ktÄ±: {output_video_temp_path}")
    return output_video_temp_path, final_summary_text

# --- Uygulama BaÅŸlangÄ±cÄ± ---
initialize_models()
load_prototypes_from_json() # JSON'dan yÃ¼kle, sonra FS taramasÄ± bunu gÃ¼ncelleyebilir/Ã¼zerine yazabilir

# --- Gradio ArayÃ¼zÃ¼ (DeÄŸiÅŸiklik Yok) ---
# ... (Gradio arayÃ¼z tanÄ±mÄ± aynÄ± kalacak) ...
# with gr.Blocks(theme=gr.themes.Monochrome(), title="YÃ¼z TanÄ±ma v4") as demo:
# ... (TÃ¼m Gradio sekme ve bileÅŸen tanÄ±mlarÄ± Ã¶ncekiyle aynÄ±) ...

# Sadece tam olmasÄ± iÃ§in Gradio arayÃ¼zÃ¼nÃ¼ tekrar ekleyelim:
with gr.Blocks(theme=gr.themes.Monochrome(), title="YÃ¼z TanÄ±ma v4") as demo:
    gr.Markdown("# ğŸ§”ğŸ½ YÃ¼z TanÄ±ma Sistemi v4 (GeliÅŸmiÅŸ YÃ¶netim ve Ayarlar)")
    
    with gr.Tab("ï¸ğŸ—ƒï¸ VeritabanÄ± YÃ¶netimi"):
        gr.Markdown("KiÅŸi fotoÄŸraflarÄ±nÄ±n bulunduÄŸu ana klasÃ¶rÃ¼ tarayarak veya manuel olarak yÃ¼z prototiplerini yÃ¶netin.")
        with gr.Row():
            with gr.Column(scale=2):
                db_path_input_manage = gr.Textbox(label="VeritabanÄ± Ana KlasÃ¶r Yolu (Dosya Sistemi TaramasÄ± Ä°Ã§in)", value=DEFAULT_DATASET_BASE_PATH)
                scan_db_button_manage = gr.Button("Dosya Sistemini Tara ve Prototipleri GÃ¼ncelle/Ekle")
                db_status_output_manage = gr.Textbox(label="Tarama Durumu", interactive=False)
            with gr.Column(scale=3):
                gr.Markdown("#### KayÄ±tlÄ± KiÅŸiler")
                enrolled_list_df_manage = gr.DataFrame(value=get_enrolled_people_dataframe, label="KayÄ±tlÄ± KiÅŸiler Listesi", interactive=False)
                with gr.Row():
                    delete_person_dropdown_manage = gr.Dropdown(choices=get_enrolled_people_names(), label="Silinecek KiÅŸiyi SeÃ§in", interactive=True)
                    delete_person_button_manage = gr.Button("SeÃ§ili KiÅŸiyi Sil")
                delete_status_output_manage = gr.Textbox(label="Silme Durumu", interactive=False)

        scan_db_button_manage.click(
            scan_filesystem_and_enroll_interface,
            inputs=[db_path_input_manage],
            outputs=[db_status_output_manage, enrolled_list_df_manage]
        ).then(lambda: gr.Dropdown(choices=get_enrolled_people_names()), outputs=delete_person_dropdown_manage)

        delete_person_button_manage.click(
            delete_person_interface,
            inputs=[delete_person_dropdown_manage],
            outputs=[delete_status_output_manage, enrolled_list_df_manage, delete_person_dropdown_manage]
        )

    with gr.Tab("ğŸ†• Manuel KiÅŸi Kaydet"):
        gr.Markdown(f"TanÄ±nacak her kiÅŸi iÃ§in **{K_SHOTS_ENROLLMENT_MANUAL} adet net yÃ¼z fotoÄŸrafÄ±** (kÄ±rpÄ±lmÄ±ÅŸ) yÃ¼kleyin.")
        person_name_manual_input_enroll = gr.Textbox(label="KiÅŸinin AdÄ± SoyadÄ±")
        with gr.Row():
            manual_img_inputs_enroll = [gr.Image(type="pil", label=f"{i+1}. YÃ¼z FotoÄŸrafÄ±", sources=["upload"]) for i in range(K_SHOTS_ENROLLMENT_MANUAL)]
        manual_enroll_button_enroll = gr.Button("Bu KiÅŸiyi Manuel Kaydet")
        manual_enroll_status_output_enroll = gr.Textbox(label="Manuel KayÄ±t Durumu", interactive=False)
        
        manual_enroll_button_enroll.click(
            enroll_person_manual_interface,
            inputs=[person_name_manual_input_enroll] + manual_img_inputs_enroll,
            outputs=[manual_enroll_status_output_enroll, enrolled_list_df_manage]
        ).then(lambda: gr.Dropdown(choices=get_enrolled_people_names()), outputs=delete_person_dropdown_manage)

    with gr.Tab("ğŸ‘ï¸â€ğŸ—¨ï¸ YÃ¼z TanÄ±ma (Resim/Video)"):
        gr.Markdown("TanÄ±ma hassasiyetini ayarlayabilir, resim, webcam veya video ile yÃ¼z tanÄ±ma yapabilirsiniz.")
        distance_threshold_slider_rec = gr.Slider(minimum=0.1, maximum=1.0, value=DEFAULT_DISTANCE_THRESHOLD, step=0.01, label="Mesafe EÅŸiÄŸi (DÃ¼ÅŸÃ¼k = Daha KatÄ± EÅŸleÅŸme)")
        
        with gr.Accordion("ğŸ“· Resimden / Webcam'den TanÄ±ma", open=True):
            img_rec_input_rec = gr.Image(type="pil", label="FotoÄŸraf YÃ¼kle / Webcam BaÅŸlat", sources=["upload", "webcam", "clipboard"])
            img_rec_button_rec = gr.Button("Resimdeki/Webcam AnlÄ±k GÃ¶rÃ¼ntÃ¼sÃ¼ndeki YÃ¼zleri TanÄ±") 
            with gr.Row():
                img_rec_output_img_rec = gr.Image(type="pil", label="TanÄ±ma Sonucu (Resim)")
                img_rec_output_summary_rec = gr.Textbox(label="TanÄ±ma Ã–zeti (Resim)", interactive=False, lines=5)
            img_rec_button_rec.click(
                 recognize_faces_image_interface,
                 inputs=[img_rec_input_rec, distance_threshold_slider_rec],
                 outputs=[img_rec_output_img_rec, img_rec_output_summary_rec]
            )

        with gr.Accordion("ğŸ“¹ Videodan TanÄ±ma", open=False):
            video_rec_input_vid = gr.Video(label="Video DosyasÄ± YÃ¼kle")
            video_rec_button_vid = gr.Button("Videodaki YÃ¼zleri TanÄ±")
            with gr.Row():
                video_rec_output_video_vid = gr.Video(label="TanÄ±ma Sonucu (Video)")
                video_rec_output_summary_vid = gr.Textbox(label="TanÄ±ma Ã–zeti (Video)", interactive=False, lines=10)
            video_rec_button_vid.click(
                recognize_faces_video_interface,
                inputs=[video_rec_input_vid, distance_threshold_slider_rec],
                outputs=[video_rec_output_video_vid, video_rec_output_summary_vid]
            )
            
    gr.Markdown("---")
    gr.Markdown(f"GeliÅŸtirici: Google Gemini & AI KullanÄ±cÄ±sÄ± | Modeller: {YOLO_FACE_DETECTION_MODEL} + DeepFace ({EMBEDDING_MODEL_NAME})")

if __name__ == '__main__':
    if not os.path.exists(DEFAULT_DATASET_BASE_PATH):
        try:
            os.makedirs(DEFAULT_DATASET_BASE_PATH)
            print(f"Ã–rnek veritabanÄ± klasÃ¶rÃ¼ '{DEFAULT_DATASET_BASE_PATH}' oluÅŸturuldu.")
        except OSError as e: print(f"'{DEFAULT_DATASET_BASE_PATH}' klasÃ¶rÃ¼ oluÅŸturulamadÄ±: {e}")
            
    demo.launch(debug=False)
